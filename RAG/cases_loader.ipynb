{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipeline - Data Ingestion to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cases_from_csv(csv_file_path):\n",
    "    \"\"\"Process cases from CSV file\"\"\"\n",
    "    # Using pandas for better control over metadata\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    all_documents = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        document = Document(\n",
    "            page_content=row['content'],\n",
    "            metadata={\n",
    "                'case_id': str(row['id']),\n",
    "                'title': row['title'],\n",
    "                'source_file': 'cases_csv',\n",
    "                'file_type': 'case',\n",
    "                'document_type': 'legal_case'\n",
    "            }\n",
    "        )\n",
    "        all_documents.append(document)\n",
    "    \n",
    "    print(f\"Processed {len(all_documents)} cases from CSV\")\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc.metadata.update({\n",
    "            'case_id': doc.metadata.get('case_id', ''),\n",
    "            'case_title': doc.metadata.get('title', ''),\n",
    "            'document_type': 'legal_case',\n",
    "            'source': 'csv_database'\n",
    "        })\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "embedding_manager = EmbeddingManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: legal_cases\n",
      "Existing documents in collection: 0\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"legal_cases\", persist_directory: str = \"./chroma_db\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Legal case embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray, batch_size: int = 5000):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store in batches of {batch_size}...\")\n",
    "\n",
    "        total_added = 0\n",
    "        \n",
    "        for batch_start in range(0, len(documents), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(documents))\n",
    "            batch_docs = documents[batch_start:batch_end]\n",
    "            batch_embeddings = embeddings[batch_start:batch_end]\n",
    "            \n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            documents_text = []\n",
    "            embeddings_list = []\n",
    "\n",
    "            for i, (doc, embedding) in enumerate(zip(batch_docs, batch_embeddings)):\n",
    "                doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{batch_start + i}\"\n",
    "                ids.append(doc_id)\n",
    "\n",
    "                metadata = dict(doc.metadata)\n",
    "                metadata['doc_index'] = batch_start + i\n",
    "                metadata['content_length'] = len(doc.page_content)\n",
    "                metadatas.append(metadata)\n",
    "\n",
    "                documents_text.append(doc.page_content)\n",
    "\n",
    "                embeddings_list.append(embedding.tolist())\n",
    "\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings_list,\n",
    "                    metadatas=metadatas,\n",
    "                    documents=documents_text\n",
    "                )\n",
    "                total_added += len(batch_docs)\n",
    "                print(f\"Successfully added batch {batch_start//batch_size + 1}: {len(batch_docs)} documents (Total: {total_added}/{len(documents)})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error adding batch {batch_start//batch_size + 1} to vector store: {e}\")\n",
    "                raise\n",
    "\n",
    "        print(f\"Completed! Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "\n",
    "vectorstore = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 710 cases from CSV\n",
      "Split 710 documents into 43713 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: J.B. PARDIWALA, J.\n",
      "For the convenience of the exposition, this judgment is divided in the following\n",
      "\n",
      "parts:\n",
      "\n",
      "8, CONSTITUENT ASSEMB. DEB., (May 30, 1949) 431.\n",
      "W.P. (C) No. 1239 of 2023                 ...\n",
      "Metadata: {'case_id': '82729634', 'title': 'The State Of Tamil Nadu vs The Governor Of Tamilnadu on 8 April, 2025', 'source_file': 'cases_csv', 'file_type': 'case', 'document_type': 'legal_case', 'case_title': 'The State Of Tamil Nadu vs The Governor Of Tamilnadu on 8 April, 2025', 'source': 'csv_database'}\n",
      "Generating embeddings for 43713 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931cb6b7a1ba427ab19c9b7b039ac55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1367 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (43713, 384)\n",
      "Adding 43713 documents to vector store in batches of 5000...\n",
      "Successfully added batch 1: 5000 documents (Total: 5000/43713)\n",
      "Successfully added batch 2: 5000 documents (Total: 10000/43713)\n",
      "Successfully added batch 3: 5000 documents (Total: 15000/43713)\n",
      "Successfully added batch 4: 5000 documents (Total: 20000/43713)\n",
      "Successfully added batch 5: 5000 documents (Total: 25000/43713)\n",
      "Successfully added batch 6: 5000 documents (Total: 30000/43713)\n",
      "Successfully added batch 7: 5000 documents (Total: 35000/43713)\n",
      "Successfully added batch 8: 5000 documents (Total: 40000/43713)\n",
      "Successfully added batch 9: 3713 documents (Total: 43713/43713)\n",
      "Completed! Total documents in collection: 43713\n"
     ]
    }
   ],
   "source": [
    "# Process cases from CSV\n",
    "case_documents = process_cases_from_csv(r\"V:\\RAG\\data\\710edited.csv\")\n",
    "\n",
    "# Split into chunks\n",
    "chunks = split_documents(case_documents)\n",
    "\n",
    "# Generate embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in vector database with batch processing\n",
    "vectorstore.add_documents(chunks, embeddings, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.3):\n",
    "        \"\"\"Retrieve relevant documents for a query\"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                # DEBUG: Show raw scores\n",
    "                print(\"=== Similarity Scores ===\")\n",
    "                for i, distance in enumerate(distances):\n",
    "                    similarity_score = 1 - (distance / 2)  # Correct conversion\n",
    "                    print(f\"Document {i+1}: distance={distance:.4f}, similarity={similarity_score:.4f}\")\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score\n",
    "                    similarity_score = 1 - (distance / 2)  # Correct conversion\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found in results\")\n",
    "                return []\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "# Reinitialize retriever with fixed class\n",
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline - VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: llama-3.1-8b-instant\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.1-8b-instant\", api_key: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable\")\n",
    "\n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"], \n",
    "            template=\"\"\"You are a legal AI assistant specializing in case law analysis. Use the following legal case context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear, informative legal answer based strictly on the context above. If the context doesn't contain relevant information, state that clearly.\"\"\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    groq_llm = GroqLLM(model_name=\"llama-3.1-8b-instant\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16117406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the main legal issue in this case?'\n",
      "Top K: 5, Score threshold: 0.5\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b75ea334ead45b4a6c18f6f9687f41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "=== Similarity Scores ===\n",
      "Document 1: distance=0.8641, similarity=0.5680\n",
      "Document 2: distance=0.8655, similarity=0.5673\n",
      "Document 3: distance=0.9002, similarity=0.5499\n",
      "Document 4: distance=0.9121, similarity=0.5440\n",
      "Document 5: distance=0.9145, similarity=0.5428\n",
      "Retrieved 5 documents (after filtering)\n",
      "The main legal issue in this case is whether the Magistrate was justified in exercising his inherent jurisdiction under Section 482 Cr.P.C. to prevent abuse of the process of court and to secure the ends of justice, particularly in a situation where the civil proceedings are pending and the dispute is primarily of a civil nature.\n"
     ]
    }
   ],
   "source": [
    "def rag_simple(query, retriever, llm, top_k=3, score_threshold=0.5):\n",
    "    \"\"\"Simple RAG function: retrieve context + generate response\"\"\"\n",
    "    # Retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=score_threshold)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    \n",
    "    if not context:\n",
    "        return \"No relevant legal context found to answer the question.\"\n",
    "\n",
    "    # Generate answer using LLM\n",
    "    prompt = f\"\"\"Use the following legal case context to answer the question concisely:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.llm.invoke([HumanMessage(content=prompt)])\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the RAG pipeline\n",
    "if groq_llm:\n",
    "    answer = rag_simple(\"What is the main legal issue in this case?\", rag_retriever, groq_llm, top_k=5, score_threshold=0.5)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
