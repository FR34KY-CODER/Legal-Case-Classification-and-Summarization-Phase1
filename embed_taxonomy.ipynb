{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a32f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name google/bigbird-roberta-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae972b7d0174aceabd150bae977ca6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\Miniconda3\\envs\\btp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vanju\\.cache\\huggingface\\hub\\models--google--bigbird-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01481ce6ada84e229c1d70331d31aa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# import toons\n",
    "\n",
    "# with open('legal_taxonomy.toon', 'r', encoding='utf-8') as f:\n",
    "#     toon_text = f.read()\n",
    "\n",
    "# taxonomy = toons.loads(toon_text)\n",
    "\n",
    "# model = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c036573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FR34K\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading taxonomy from legal_taxonomy.toon...\n",
      "Embedding taxonomy categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:02<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving updated taxonomy to legal_taxonomy_with_embeddings.toon...\n",
      "✅ Done.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import toons\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Added for a progress bar\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = 'BAAI/bge-small-en-v1.5'\n",
    "MAX_TOKENS = 512 # Max sequence length for this model\n",
    "TOON_INPUT = 'legal_taxonomy.toon'\n",
    "TOON_OUTPUT = 'legal_taxonomy_with_embeddings.toon'\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "# Load model directly to GPU\n",
    "model = SentenceTransformer(MODEL_NAME, device='cuda')\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# --- Helper Function for Chunking ---\n",
    "def chunk_text(text, max_tokens=MAX_TOKENS):\n",
    "    \"\"\"Chunks text into pieces smaller than the model's max token limit.\"\"\"\n",
    "    if not text:\n",
    "        return [\"\"]  # Handle empty descriptions/keywords\n",
    "    \n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    \n",
    "    if not tokens:\n",
    "        return [\"\"] # Handle cases where tokenizer returns empty\n",
    "\n",
    "    for i in range(0, len(tokens), max_tokens):\n",
    "        chunk_ids = tokens[i:i + max_tokens]\n",
    "        chunks.append(tokenizer.decode(chunk_ids))\n",
    "    return chunks\n",
    "\n",
    "# --- Main Embedding Function (Modified) ---\n",
    "def add_embeddings_to_taxonomy(taxonomy, model):\n",
    "    \"\"\"\n",
    "    Iterates through the taxonomy, creates an embedding for each category\n",
    "    using a chunking and averaging strategy, and adds it to the data.\n",
    "    \"\"\"\n",
    "    print(\"Embedding taxonomy categories...\")\n",
    "    # Use tqdm for a nice progress bar\n",
    "    for category_name, category_data in tqdm(taxonomy.items()):\n",
    "\n",
    "        # 1. Combine Description and Keywords\n",
    "        desc = category_data.get('Description', '')\n",
    "        \n",
    "        # Find the keyword field (handles 'Keywords' or 'Keywords_1', etc.)\n",
    "        kw_field = next((k for k in category_data.keys() if k.startswith('Keywords')), None)\n",
    "        keywords_value = category_data.get(kw_field, '') if kw_field else ''\n",
    "        \n",
    "        if isinstance(keywords_value, list):\n",
    "            keywords_text = ' '.join(keywords_value)\n",
    "        else:\n",
    "            keywords_text = str(keywords_value)\n",
    "\n",
    "        text_to_embed = f\"{desc} {keywords_text}\".strip()\n",
    "\n",
    "        # 2. Apply Chunking Strategy\n",
    "        chunks = chunk_text(text_to_embed)\n",
    "        \n",
    "        # 3. Encode chunks (this runs on the GPU)\n",
    "        chunk_embs = model.encode(chunks, show_progress_bar=False)\n",
    "\n",
    "        # 4. Average embeddings\n",
    "        if len(chunk_embs) == 1:\n",
    "            final_embedding = chunk_embs[0]\n",
    "        else:\n",
    "            # Stack all chunk embeddings and calculate the mean\n",
    "            final_embedding = np.mean(np.vstack(chunk_embs), axis=0)\n",
    "\n",
    "        # 5. Store the final embedding\n",
    "        category_data['embeddings'] = final_embedding.tolist()\n",
    "\n",
    "    return taxonomy\n",
    "\n",
    "# --- Execution ---\n",
    "print(f\"Loading taxonomy from {TOON_INPUT}...\")\n",
    "with open(TOON_INPUT, 'r', encoding='utf-8') as f:\n",
    "    toon_text = f.read()\n",
    "taxonomy = toons.loads(toon_text)\n",
    "\n",
    "# Run the main function\n",
    "taxonomy_with_embeddings = add_embeddings_to_taxonomy(taxonomy, model)\n",
    "\n",
    "print(f\"\\nSaving updated taxonomy to {TOON_OUTPUT}...\")\n",
    "updated_toon_text = toons.dumps(taxonomy_with_embeddings)\n",
    "with open(TOON_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "    f.write(updated_toon_text)\n",
    "\n",
    "print(\"✅ Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_embeddings_to_taxonomy(taxonomy, model):\n",
    "    \n",
    "#     for category_name, category_data in taxonomy.items():\n",
    "\n",
    "#         desc = category_data.get('Description', '')\n",
    "\n",
    "#         kw_field = next((k for k in category_data.keys() if k.startswith('Keywords')), None)\n",
    "#         keywords_value = category_data.get(kw_field, '') if kw_field else ''\n",
    "        \n",
    "#         if isinstance(keywords_value, list):\n",
    "#             keywords_text = ' '.join(keywords_value)\n",
    "#         else:\n",
    "#             keywords_text = str(keywords_value)\n",
    "\n",
    "#         text_to_embed = f\"{desc} {keywords_text}\".strip()\n",
    "\n",
    "#         embedding = model.encode(text_to_embed)\n",
    "\n",
    "#         category_data['embeddings'] = embedding.tolist()\n",
    "\n",
    "#     return taxonomy\n",
    "\n",
    "# taxonomy_with_embeddings = add_embeddings_to_taxonomy(taxonomy, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated_toon_text = toons.dumps(taxonomy_with_embeddings)\n",
    "\n",
    "# with open('legal_taxonomy_with_embeddings.toon', 'w', encoding='utf-8') as f:\n",
    "#     f.write(updated_toon_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
